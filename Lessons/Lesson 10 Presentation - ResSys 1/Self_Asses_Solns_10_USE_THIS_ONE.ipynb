{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# execute to import notebook styling for tables and width etc.\n",
    "from IPython.core.display import HTML\n",
    "import urllib.request\n",
    "response = urllib.request.urlopen('https://raw.githubusercontent.com/DataScienceUWL/DS775v2/master/ds755.css')\n",
    "HTML(response.read().decode(\"utf-8\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=18>Lesson 10 - Self-Assessment Solutions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Self-Assessment: Load and Display - Solution</font>\n",
    "There's nothing too new here. You've done this kind of work before. What's more important here than the code is making sure you take a minute or two to understand the data you're pulling in. What columns do you have available to you? Which columns contain simple values and which columns contain lists. Think about how you could or couldn't use this data to make recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ted = pd.read_csv('./data/ted-talks/ted_main.csv')\n",
    "ted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Self-Assessment: Pandas - Solution </font>\n",
    "Remember that shape gives you the number of rows first, followed by the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2550 TED talks in this data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Self-Assessment: Prerequisites - Solution </font> \n",
    "\n",
    "Remember that when you're calculating the quantile for some piece of data, you'll get different results if you calculate it before or after you do your other subsetting. First, let's calculate the views quantile before we figure the rest of our prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of views for the 10th percentile - calculated from the whole dataframe\n",
    "m = ted['views'].quantile(0.10)\n",
    "\n",
    "#Only consider talks of at least 5 minutes\n",
    "q_talks = ted[(ted['duration'] >= 300)]\n",
    "\n",
    "#Only consider talks with one speaker\n",
    "q_talks = q_talks[q_talks['num_speaker']==1]\n",
    "\n",
    "#Only consider talks in the top 90%\n",
    "q_talks = q_talks[q_talks['views'] >= m]\n",
    "\n",
    "#Inspect the number of talks that made the cut\n",
    "q_talks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare that with calculating the quantile after we subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only consider talks of at least 5 minutes\n",
    "q_talks2 = ted[(ted['duration'] >= 300)]\n",
    "\n",
    "#Only consider talks with one speaker\n",
    "q_talks2 = q_talks2[q_talks2['num_speaker']==1]\n",
    "\n",
    "#Calculate the number of views for the 10th percentile - calculated from the whole dataframe\n",
    "m2 = q_talks2['views'].quantile(0.10)\n",
    "\n",
    "#Only consider talks in the top 90%\n",
    "q_talks2 = q_talks2[q_talks2['views'] >= m2]\n",
    "\n",
    "#Inspect the number of talks that made the cut\n",
    "q_talks2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no universally \"right\" answer as to whether you should calculate the quantile before or after you've narrowed the initial dataset. It depends on what you're trying to accomplish. If you want the most viewed talks *that meet your criteria* you'd calculate it after you've subsetted. If you want the most viewed talks *overall* you'd calculate it before you've subsetted.\n",
    "\n",
    "For our homework, we'll typically calculate the quantile on the entire data set, to keep consistent with the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Self-Assessment: Compute a Metric, Sort and Print - Solution </font>  \n",
    "\n",
    "Note that here we are computing our metric on our narrowed data set. We could have created the metric on the entire dataset. But, if we know that we're only interested in a portion of the talks, we should narrow our dataset before computing the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the metric of the comments to views ratio\n",
    "q_talks['comments_per_1000views']=1000*q_talks['comments']/q_talks['views']\n",
    "\n",
    "#Sort talks in descending order of the ratio of views to comments\n",
    "q_talks = q_talks.sort_values('comments_per_1000views', ascending=False)\n",
    "\n",
    "#Print the top 10 talks\n",
    "q_talks[['description', 'main_speaker', 'comments_per_1000views']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Self-Assessment: Dealing with Dates - Solution </font>\n",
    "This is straight out of the book. Apply is a handy function available in pandas that lets you run a function for each row or column of your data. You're seeing examples here of using a lambda (inline) function as well as using a separately created function (convert_int). \n",
    "\n",
    "The lambda function is just grabbing the year from the published date. It's doing that by splitting the string on the '-' character. This creates an array. We grab the first item in the array, which, if we had a valid date, should be the year. If we didn't have a valid date, then we drop in the np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert release_date into pandas datetime format\n",
    "ted['published_date'] = pd.to_datetime(ted['published_date'],\n",
    "                                       errors='coerce', unit='s')\n",
    "\n",
    "#see what the new date looks like\n",
    "print(\"This is what the datetime string looks like:\")\n",
    "display(ted['published_date'].head())\n",
    "\n",
    "\n",
    "#Extract year from the datetime\n",
    "ted['published_year'] = ted['published_date'].apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)\n",
    "\n",
    "#Helper function to convert NaT to 0 and all other years to integers.\n",
    "def convert_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "#Apply convert_int to the year feature\n",
    "ted['published_year'] = ted['published_year'].apply(convert_int)\n",
    "    \n",
    "ted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Self-Assessment: Stringified Dictionaries - Solution </font>\n",
    "\n",
    "This is also straight from the book. When we use the literal_eval function on the ratings column, we get a dictionary that we can manipulate. The \"name\" key holds the part of the ratings that we care about. We want to convert these words to lower case and create a list of the words. We create an empty list if there were no ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the literal_eval function from ast\n",
    "from ast import literal_eval\n",
    "\n",
    "#Convert all NaN into stringified empty lists\n",
    "ted['ratings'] = ted['ratings'].fillna('[]')\n",
    "\n",
    "#Apply literal_eval to convert stringified empty lists to the list object\n",
    "ted['ratings'] = ted['ratings'].apply(literal_eval)\n",
    "\n",
    "\n",
    "#Convert list of dictionaries to a list of strings\n",
    "ted['ratings'] = ted['ratings'].apply(lambda x: [i['name'].lower() for i in x] if isinstance(x, list) else [])\n",
    "\n",
    "#See how 'ratings' has changed?\n",
    "ted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Exploding\" creates a row for each word in our ratings list. This obviously creates duplicate data, and if you create more metrics after you've exploded, you're going to get strange results. In the case of our knowledge-based recommender, since we're only asking you to return the results for a single word, it's not that big of a deal. But we'll show you two approaches to accomplish the same thing. First, let's explode, but not join our exploded column back to our main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new feature by exploding word ratings\n",
    "s = ted.apply(lambda x: pd.Series(x['ratings']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "\n",
    "#look at what we have\n",
    "s.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Self-Assessment: Create the Knowledge-Based Recommender - Solution </font>\n",
    "\n",
    "We're creating this as a function that takes in the dataframe and the percentile of views that we want to return. \n",
    "We'll first generate our list of unique words to present to users. We'll also stringify our list of ratings so we can use str.contains to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chart(gen_ted, percentile=0.1):\n",
    "    #Show user the list of word ratings to choose from\n",
    "    s = gen_ted.apply(lambda x: pd.Series(x['ratings']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "    print(s.unique())\n",
    "    \n",
    "    #convert our ratings to strings\n",
    "    gen_ted['ratings'] = gen_ted['ratings'].apply(', '.join)\n",
    "    \n",
    "    #Ask for preferred word rating\n",
    "    print(\"Select a descriptive word from the list above for the 'word rating'\")\n",
    "    rating = input()\n",
    "    \n",
    "    #Ask for lower limit of film year\n",
    "    print(\"Input earliest year published (2006 to 2017)\")\n",
    "    low_year = int(input())\n",
    "    \n",
    "    #Ask for upper limit of film year\n",
    "    print(\"Input latest year published(2006 to 2017)\")\n",
    "    high_year = int(input())\n",
    "    \n",
    "    \n",
    "    #Define a new talks variable to store the preferred talks. \n",
    "    #Copy the contents of gen_ted to talks\n",
    "    talks = gen_ted.copy()\n",
    "    \n",
    "    #Filter based on the condition\n",
    "    talks = talks[(talks['ratings'].str.contains(rating)) & \n",
    "                    (talks['published_year'] >= low_year) & \n",
    "                    (talks['published_year'] <= high_year)]\n",
    "    \n",
    "    #Calculate the number of views for the 10th percentile \n",
    "    m = ted['views'].quantile(percentile)\n",
    "\n",
    "    #Only consider movies that have higher than m votes. Save this in a new dataframe q_movies\n",
    "    q_talks = talks.copy().loc[talks['views'] >= m]\n",
    "    \n",
    "    #create the metric of the comments to views ratio\n",
    "    q_talks['comments_per_1000views']=1000*q_talks['comments']/q_talks['views']\n",
    "\n",
    "    #Sort talks in descending order of the ratio of views to comments\n",
    "    q_talks = q_talks.sort_values('comments_per_1000views', ascending=False)\n",
    "    \n",
    "    return q_talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funny' 'beautiful' 'ingenious' 'courageous' 'longwinded' 'confusing'\n",
      " 'informative' 'fascinating' 'unconvincing' 'persuasive' 'jaw-dropping'\n",
      " 'ok' 'obnoxious' 'inspiring']\n",
      "Select a descriptive word from the list above for the 'word rating'\n",
      "obnoxious\n",
      "Input earliest year published (2006 to 2017)\n",
      "2009\n",
      "Input latest year published(2006 to 2017)\n",
      "2014\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>published_year</th>\n",
       "      <th>comments_per_1000views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>David Bismark</td>\n",
       "      <td>David Bismark: E-voting without fraud</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.534355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>Sharmeen Obaid-Chinoy</td>\n",
       "      <td>Sharmeen Obaid-Chinoy: Inside a school for sui...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.420683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>Janet Echelman</td>\n",
       "      <td>Janet Echelman: Taking imagination seriously</td>\n",
       "      <td>2011</td>\n",
       "      <td>1.359572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>Lesley Hazleton</td>\n",
       "      <td>Lesley Hazleton: On reading the Koran</td>\n",
       "      <td>2011</td>\n",
       "      <td>1.285149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1787</td>\n",
       "      <td>David Chalmers</td>\n",
       "      <td>David Chalmers: How do you explain consciousness?</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.235918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               main_speaker  \\\n",
       "803           David Bismark   \n",
       "694   Sharmeen Obaid-Chinoy   \n",
       "954          Janet Echelman   \n",
       "840         Lesley Hazleton   \n",
       "1787         David Chalmers   \n",
       "\n",
       "                                                   name  published_year  \\\n",
       "803               David Bismark: E-voting without fraud            2010   \n",
       "694   Sharmeen Obaid-Chinoy: Inside a school for sui...            2010   \n",
       "954        Janet Echelman: Taking imagination seriously            2011   \n",
       "840               Lesley Hazleton: On reading the Koran            2011   \n",
       "1787  David Chalmers: How do you explain consciousness?            2014   \n",
       "\n",
       "      comments_per_1000views  \n",
       "803                 1.534355  \n",
       "694                 1.420683  \n",
       "954                 1.359572  \n",
       "840                 1.285149  \n",
       "1787                1.235918  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate the chart for top talks for these user preferences and display top 5.\n",
    "gen_ted_final = build_chart(ted).head(5)\n",
    "\n",
    "gen_ted_final[['main_speaker','name','published_year','comments_per_1000views']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Self-Assessment: TF-IDF Vectors - Solution</font>\n",
    "This is all straight from the book. More information about the TfidfVectorizer is available online here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2550, 15162)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import TfIdfVectorizer from the scikit-learn library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Define a TF-IDF Vectorizer Object. Remove all english stopwords\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Replace NaN with an empty string\n",
    "ted['description'] = ted['description'].fillna('')\n",
    "\n",
    "#Construct the required TF-IDF matrix by applying the fit_transform method on the description feature\n",
    "tfidf_matrix = tfidf.fit_transform(ted['description'])\n",
    "\n",
    "#Output the shape of tfidf_matrix (rows first, then columns)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['albatrosses',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'alberto',\n",
       " 'albinism',\n",
       " 'albright',\n",
       " 'album',\n",
       " 'albuquerque',\n",
       " 'alchemists',\n",
       " 'alcohol']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bonus - take a look some of the individual words in the description\n",
    "feature_names = tfidf.get_feature_names()\n",
    "feature_names[500:510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bonus - this is saying that for the first document, none of the first 10 words shown above show up in that document\n",
    "tfidf_list = tfidf_matrix.toarray()\n",
    "tfidf_list[0, 500:510]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"blue\"> Self-Assessment: Create the Content-Based Recommender Based on Cosine Similarity - Solution </font>\n",
    "This is also straight from the book. We don't expect you to understand everything to do with linear kernels. But if you're interested, the documentation is here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linear_kernel to compute the dot product\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "#Construct a reverse mapping of indices and talk names, and drop duplicate names, if any\n",
    "indices = pd.Series(ted.index, index=ted['name']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in talk title as input and gives recommendations \n",
    "def content_recommender(name, cosine_sim=cosine_sim, df=ted, indices=indices):\n",
    "    # Obtain the index of the talks that matches the title\n",
    "    idx = indices[name]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all talks with that name\n",
    "    # And convert it into a list of tuples as described above\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the talks based on the cosine similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar talks. Ignore the first talk.\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return df['name'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1910    Tom Wujec: Got a wicked problem? First, tell m...\n",
       "451     Dan Ariely: Are we in control of our own decis...\n",
       "1115         Mikko Hypponen: Three types of online attack\n",
       "1405           Ronny Edry: Israel and Iran: A love story?\n",
       "2361     Sisonke Msimang: If a story moves you, act on it\n",
       "406                      Dan Ariely: Our buggy moral code\n",
       "775         Julian Treasure: Shh! Sound health in 8 steps\n",
       "335     Samantha Power: A complicated hero in the war ...\n",
       "145                  John Maeda: Designing for simplicity\n",
       "757     His Holiness the Karmapa: The technology of th...\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get recommendations for Tyler Cowen: Be suspicious of simple stories\n",
    "content_recommender('Tyler Cowen: Be suspicious of simple stories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.517px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
